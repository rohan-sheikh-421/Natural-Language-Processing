{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict\n",
        "#----------------------------------------\n",
        "#  Data input\n",
        "#----------------------------------------\n",
        "\n",
        "# Read a text file into a corpus (list of sentences (which in turn are lists of words))\n",
        "# (taken from nested section of HW0)\n",
        "def readFileToCorpus(f):\n",
        "    \"\"\" Reads in the text file f which contains one sentence per line.\n",
        "    \"\"\"\n",
        "    if os.path.isfile(f):\n",
        "        file = open(f, \"r\") # open the input file in read-only mode\n",
        "        i = 0 # this is just a counter to keep track of the sentence numbers\n",
        "        corpus = [] # this will become a list of sentences\n",
        "        print(\"Reading file \", f)\n",
        "        for line in file:\n",
        "            i += 1\n",
        "            sentence = line.split() # split the line into a list of words\n",
        "            #append this lis as an element to the list of sentences\n",
        "            corpus.append(sentence)\n",
        "            if i % 1000 == 0:\n",
        "    \t#print a status message: str(i) turns int i into a string\n",
        "    \t#so we can concatenate it\n",
        "                sys.stderr.write(\"Reading sentence \" + str(i) + \"\\n\")\n",
        "        #endif\n",
        "    #endfor\n",
        "        return corpus\n",
        "    else:\n",
        "    #ideally we would throw an exception here, but this will suffice\n",
        "        print(\"Error: corpus file \", f, \" does not exist\")\n",
        "        sys.exit() # exit the script\n",
        "    #endif\n",
        "#enddef\n",
        "\n",
        "\n",
        "# Preprocess the corpus\n",
        "def preprocess(corpus):\n",
        "    #find all the rare words\n",
        "    freqDict = defaultdict(int)\n",
        "    for sen in corpus:\n",
        "\t    for word in sen:\n",
        "\t       freqDict[word] += 1\n",
        "\t#endfor\n",
        "    #endfor\n",
        "\n",
        "    #replace rare words with unk\n",
        "    for sen in corpus:\n",
        "        for i in range(0, len(sen)):\n",
        "            word = sen[i]\n",
        "            print(word)\n",
        "            print(freqDict[word])\n",
        "            if freqDict[word] < 2:\n",
        "\n",
        "                sen[i] = UNK\n",
        "\t    #endif\n",
        "\t#endfor\n",
        "    #endfor\n",
        "\n",
        "    #bookend the sentences with start and end tokens\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    #endfor\n",
        "\n",
        "    return corpus\n",
        "#enddef\n",
        "\n",
        "def preprocessTest(vocab, corpus):\n",
        "    #replace test words that were unseen in the training with unk\n",
        "    for sen in corpus:\n",
        "        for i in range(0, len(sen)):\n",
        "            word = sen[i]\n",
        "            if word not in vocab:\n",
        "                sen[i] = UNK\n",
        "\t    #endif\n",
        "\t#endfor\n",
        "    #endfor\n",
        "\n",
        "    #bookend the sentences with start and end tokens\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    #endfor\n",
        "\n",
        "    return corpus\n",
        "#enddef\n",
        "\n",
        "# Constants\n",
        "UNK = \"UNK\"     # Unknown word token\n",
        "start = \"<s>\"   # Start-of-sentence token\n",
        "end = \"</s>\"    # End-of-sentence-token\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# Language models and data structures\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "# Parent class for the three language models you need to implement\n",
        "class LanguageModel:\n",
        "    # Initialize and train the model (ie, estimate the model's underlying probability\n",
        "    # distribution from the training corpus)\n",
        "    def __init__(self, corpus):\n",
        "        print(\"\"\"Your task is to implement four kinds of n-gram language models:\n",
        "      a) an (unsmoothed) unigram model (UnigramModel)\n",
        "      b) a unigram model smoothed using Laplace smoothing (SmoothedUnigramModel)\n",
        "      c) an unsmoothed bigram model (BigramModel)\n",
        "      d) a bigram model smoothed using linear interpolation smoothing (SmoothedBigramModelInt)\n",
        "      \"\"\")\n",
        "    #enddef\n",
        "\n",
        "    # Generate a sentence by drawing words according to the\n",
        "    # model's probability distribution\n",
        "    # Note: think about how to set the length of the sentence\n",
        "    #in a principled way\n",
        "    def generateSentence(self):\n",
        "        print(\"Implement the generateSentence method in each subclass\")\n",
        "        return \"mary had a little lamb .\"\n",
        "    #emddef\n",
        "\n",
        "    # Given a sentence (sen), return the probability of\n",
        "    # that sentence under the model\n",
        "    def getSentenceProbability(self, sen):\n",
        "        print(\"Implement the getSentenceProbability method in each subclass\")\n",
        "        return 0.0\n",
        "    #enddef\n",
        "\n",
        "    # Given a corpus, calculate and return its perplexity\n",
        "    #(normalized inverse log probability)\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        print(\"Implement the getCorpusPerplexity method\")\n",
        "        return 0.0\n",
        "    #enddef\n",
        "\n",
        "    # Given a file (filename) and the number of sentences, generate a list\n",
        "    # of sentences and write each to file along with its model probability.\n",
        "    # Note: you shouldn't need to change this method\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(0,numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "\n",
        "\t#endfor\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "# Unigram language model\n",
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        print(\"Subtask: implement the unsmoothed unigram language model\")\n",
        "    #endddef\n",
        "#endclass\n",
        "\n",
        "#Smoothed unigram language model (use laplace for smoothing)\n",
        "class SmoothedUnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        print(\"Subtask: implement the smoothed unigram language model\")\n",
        "    #endddef\n",
        "#endclass\n",
        "\n",
        "# Unsmoothed bigram language model\n",
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        print(\"Subtask: implement the unsmoothed bigram language model\")\n",
        "    #endddef\n",
        "#endclass\n",
        "\n",
        "\n",
        "\n",
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "#----------------------------------------\n",
        "# Data input\n",
        "#----------------------------------------\n",
        "\n",
        "# Read a text file into a corpus (list of sentences (which in turn are lists of words))\n",
        "def readFileToCorpus(f):\n",
        "    \"\"\" Reads in the text file f which contains one sentence per line. \"\"\"\n",
        "    if os.path.isfile(f):\n",
        "        file = open(f, \"r\")  # open the input file in read-only mode\n",
        "        i = 0  # this is just a counter to keep track of the sentence numbers\n",
        "        corpus = []  # this will become a list of sentences\n",
        "        print(\"Reading file \", f)\n",
        "        for line in file:\n",
        "            i += 1\n",
        "            sentence = line.split()  # split the line into a list of words\n",
        "            corpus.append(sentence)\n",
        "            if i % 1000 == 0:\n",
        "                sys.stderr.write(\"Reading sentence \" + str(i) + \"\\n\")\n",
        "            # endfor\n",
        "        return corpus\n",
        "    else:\n",
        "        print(\"Error: corpus file \", f, \" does not exist\")\n",
        "        sys.exit()  # exit the script\n",
        "    # endif\n",
        "\n",
        "# Preprocess the corpus\n",
        "def preprocess(corpus):\n",
        "    freqDict = defaultdict(int)\n",
        "    for sen in corpus:\n",
        "        for word in sen:\n",
        "            freqDict[word] += 1\n",
        "    for sen in corpus:\n",
        "        for i in range(0, len(sen)):\n",
        "            word = sen[i]\n",
        "            if freqDict[word] < 2:\n",
        "                sen[i] = UNK\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    return corpus\n",
        "\n",
        "# Constants\n",
        "UNK = \"UNK\"  # Unknown word token\n",
        "start = \"<s>\"  # Start-of-sentence token\n",
        "end = \"</s>\"  # End-of-sentence-token\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# Language models and data structures\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "# Parent class for the four language models you need to implement\n",
        "class LanguageModel:\n",
        "    # Initialize and train the model (ie, estimate the model's underlying probability\n",
        "    # distribution from the training corpus)\n",
        "    def __init__(self, corpus):\n",
        "        self.corpus = preprocess(corpus)\n",
        "        self.vocab = set([word for sen in self.corpus for word in sen])\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.unigram_counts = Counter([word for sentence in self.corpus for word in sentence])\n",
        "        self.bigram_counts = Counter([(sentence[i], sentence[i+1]) for sentence in self.corpus for i in range(len(sentence)-1)])\n",
        "        self.total_words = sum(self.unigram_counts.values())\n",
        "        self.total_bigrams = sum(self.bigram_counts.values())\n",
        "\n",
        "    # Generate a sentence by drawing words according to the\n",
        "    # model's probability distribution\n",
        "    # Note: think about how to set the length of the sentence\n",
        "    #in a principled way\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        while sentence[-1] != end:\n",
        "            word = self.drawWord(sentence)\n",
        "            sentence.append(word)\n",
        "        return sentence\n",
        "\n",
        "    # Given a sentence (sen), return the probability of\n",
        "    # that sentence under the model\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1\n",
        "        for i in range(len(sen)-1):\n",
        "            bigram_count = self.bigram_counts[(sen[i], sen[i+1])]\n",
        "            unigram_count = self.unigram_counts[sen[i]]\n",
        "            if bigram_count > 0:\n",
        "                probability *= bigram_count / unigram_count\n",
        "            else:\n",
        "                probability *= 0.4 * unigram_count / self.total_words\n",
        "        return probability\n",
        "\n",
        "    # Given a corpus, calculate and return its perplexity\n",
        "    #(normalized inverse log probability)\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        corpus = preprocessTest(self.vocab, corpus)\n",
        "        log_sum = 0\n",
        "        total_words = 0\n",
        "        for sentence in corpus:\n",
        "            total_words += len(sentence) - 1\n",
        "            log_prob = math.log(self.getSentenceProbability(sentence))\n",
        "            log_sum += log_prob\n",
        "        perplexity = math.exp(-log_sum / total_words)\n",
        "        return perplexity\n",
        "\n",
        "    # Generate a single random word according to the distribution\n",
        "    def drawWord(self, sentence):\n",
        "        rand = random.random()\n",
        "        prob_sum = 0\n",
        "        for word in self.vocab:\n",
        "            prob_sum += self.getBigramProbability(sentence[-1], word)\n",
        "            if rand <= prob_sum:\n",
        "                return word\n",
        "        return end\n",
        "\n",
        "    # Given two words, return the probability of the second word\n",
        "    # given the first word\n",
        "    def getBigramProbability(self, word1, word2):\n",
        "        bigram_count = self.bigram_counts[(word1, word2)]\n",
        "        unigram_count = self.unigram_counts[word1]\n",
        "        if bigram_count > 0:\n",
        "            return bigram_count / unigram_count\n",
        "        else:\n",
        "            return 0.4 * unigram_count / self.total_words\n",
        "\n",
        "# Unigram language model\n",
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        while sentence[-1] != end:\n",
        "            word = self.drawWord(sentence)\n",
        "            sentence.append(word)\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1\n",
        "        for i in range(len(sen)-1):\n",
        "            unigram_count = self.unigram_counts[sen[i]]\n",
        "            probability *= unigram_count / self.total_words\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "        filePointer.close()\n",
        "\n",
        "    def drawWord(self, sentence):\n",
        "        rand = random.random()\n",
        "        prob_sum = 0\n",
        "        for word in self.vocab:\n",
        "            prob_sum += self.unigram_counts[word] / self.total_words\n",
        "            if rand <= prob_sum:\n",
        "                return word\n",
        "        return end\n",
        "\n",
        "# Smoothed unigram language model (use laplace for smoothing)\n",
        "class SmoothedUnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        while sentence[-1] != end:\n",
        "            word = self.drawWord(sentence)\n",
        "            sentence.append(word)\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1\n",
        "        for i in range(len(sen)-1):\n",
        "            unigram_count = self.unigram_counts[sen[i]]\n",
        "            probability *= (unigram_count + 1) / (self.total_words + self.vocab_size)\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "        filePointer.close()\n",
        "\n",
        "    def drawWord(self, sentence):\n",
        "        rand = random.random()\n",
        "        prob_sum = 0\n",
        "        for word in self.vocab:\n",
        "            prob_sum += (self.unigram_counts[word] + 1) / (self.total_words + self.vocab_size)\n",
        "            if rand <= prob_sum:\n",
        "                return word\n",
        "        return end\n",
        "\n",
        "# Unsmoothed bigram language model\n",
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        while sentence[-1] != end:\n",
        "            word = self.drawWord(sentence)\n",
        "            sentence.append(word)\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1\n",
        "        for i in range(len(sen)-1):\n",
        "            bigram_count = self.bigram_counts[(sen[i], sen[i+1])]\n",
        "            unigram_count = self.unigram_counts[sen[i]]\n",
        "            if bigram_count > 0:\n",
        "                probability *= bigram_count / unigram_count\n",
        "            else:\n",
        "                probability *= 0.4 * unigram_count / self.total_words\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "        filePointer.close()\n",
        "\n",
        "    def drawWord(self, sentence):\n",
        "        rand = random.random()\n",
        "        prob_sum = 0\n",
        "        for word in self.vocab:\n",
        "            prob_sum += self.getBigramProbability(sentence[-1], word)\n",
        "            if rand <= prob_sum:\n",
        "                return word\n",
        "        return end\n",
        "\n",
        "# Smoothed bigram language model (use linear interpolation for smoothing, set lambda1 = lambda2 = 0.5)\n",
        "class SmoothedBigramModelInt(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "        self.lambda1 = 0.5\n",
        "        self.lambda2 = 0.5\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        while sentence[-1] != end:\n",
        "            word = self.drawWord(sentence)\n",
        "            sentence.append(word)\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1\n",
        "        for i in range(len(sen)-1):\n",
        "            probability *= self.getSmoothedBigramProbability(sen[i], sen[i+1])\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "        filePointer.close()\n",
        "\n",
        "    def drawWord(self, sentence):\n",
        "        rand = random.random()\n",
        "        prob_sum = 0\n",
        "        for word in self.vocab:\n",
        "            prob_sum += self.getSmoothedBigramProbability(sentence[-1], word)\n",
        "            if rand <= prob_sum:\n",
        "                return word\n",
        "        return end\n",
        "\n",
        "    def getSmoothedBigramProbability(self, word1, word2):\n",
        "        bigram_count = self.bigram_counts[(word1, word2)]\n",
        "        unigram_count = self.unigram_counts[word1]\n",
        "        unigram_total = sum(self.unigram_counts.values())\n",
        "\n",
        "        probability = (self.lambda1 * (bigram_count / unigram_count)) + (self.lambda2 * (unigram_count / unigram_total))\n",
        "        return probability\n",
        "\n",
        "\n",
        "# Sample class for a unsmoothed unigram probability distribution\n",
        "# Note:\n",
        "#       Feel free to use/re-use/modify this class as necessary for your\n",
        "#       own code (e.g. converting to log probabilities after training).\n",
        "#       This class is intended to help you get started\n",
        "#       with your implementation of the language models above.\n",
        "class UnigramDist:\n",
        "    def __init__(self, corpus):\n",
        "        self.counts = defaultdict(float)\n",
        "        self.total = 0.0\n",
        "        self.train(corpus)\n",
        "    #endddef\n",
        "\n",
        "    # Add observed counts from corpus to the distribution\n",
        "    def train(self, corpus):\n",
        "        for sen in corpus:\n",
        "            for word in sen:\n",
        "                if word == start:\n",
        "                    continue\n",
        "                self.counts[word] += 1.0\n",
        "                self.total += 1.0\n",
        "            #endfor\n",
        "        #endfor\n",
        "    #enddef\n",
        "\n",
        "    # Returns the probability of word in the distribution\n",
        "    def prob(self, word):\n",
        "        return self.counts[word]/self.total\n",
        "    #enddef\n",
        "\n",
        "    # Generate a single random word according to the distribution\n",
        "    def draw(self):\n",
        "        rand = random.random()\n",
        "        for word in self.counts.keys():\n",
        "            rand -= self.prob(word)\n",
        "            if rand <= 0.0:\n",
        "                return word\n",
        "\t    #endif\n",
        "\t#endfor\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "#-------------------------------------------\n",
        "# The main routine\n",
        "#-------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    trainCorpus = readFileToCorpus('train.txt')\n",
        "    trainCorpus = preprocess(trainCorpus)\n",
        "\n",
        "    posTestCorpus = readFileToCorpus('pos_test.txt')\n",
        "    negTestCorpus = readFileToCorpus('neg_test.txt')\n",
        "\n",
        "    vocab = set()\n",
        "    for sentence in trainCorpus:\n",
        "        vocab.update(sentence)\n",
        "\n",
        "    posTestCorpus = preprocessTest(vocab, posTestCorpus)\n",
        "    negTestCorpus = preprocessTest(vocab, negTestCorpus)\n",
        "\n",
        "    # Creating language models\n",
        "    unigram_model = UnigramModel(trainCorpus)\n",
        "    smoothed_unigram_model = SmoothedUnigramModel(trainCorpus)\n",
        "    bigram_model = BigramModel(trainCorpus)\n",
        "    smoothed_bigram_model = SmoothedBigramModelInt(trainCorpus)\n",
        "\n",
        "    # Generating sentences and writing to files\n",
        "    unigram_model.generateSentencesToFile(20, 'unigram_output.txt')\n",
        "    smoothed_unigram_model.generateSentencesToFile(20, 'smooth_unigram_output.txt')\n",
        "    bigram_model.generateSentencesToFile(20, 'bigram_output.txt')\n",
        "    smoothed_bigram_model.generateSentencesToFile(20, 'smooth_bigram_output.txt')\n",
        "\n",
        "    # Calculating perplexity\n",
        "    unigram_perplexity_pos = unigram_model.getCorpusPerplexity(posTestCorpus)\n",
        "    unigram_perplexity_neg = unigram_model.getCorpusPerplexity(negTestCorpus)\n",
        "\n",
        "    smoothed_unigram_perplexity_pos = smoothed_unigram_model.getCorpusPerplexity(posTestCorpus)\n",
        "    smoothed_unigram_perplexity_neg = smoothed_unigram_model.getCorpusPerplexity(negTestCorpus)\n",
        "\n",
        "    bigram_perplexity_pos = bigram_model.getCorpusPerplexity(posTestCorpus)\n",
        "    bigram_perplexity_neg = bigram_model.getCorpusPerplexity(negTestCorpus)\n",
        "\n",
        "    smoothed_bigram_perplexity_pos = smoothed_bigram_model.getCorpusPerplexity(posTestCorpus)\n",
        "    smoothed_bigram_perplexity_neg = smoothed_bigram_model.getCorpusPerplexity(negTestCorpus)\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"UNIGRAM : \")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Unigram Perplexity (Positive Test Corpus):\", unigram_perplexity_pos)\n",
        "    print(\"Unigram Perplexity (Negative Test Corpus):\", unigram_perplexity_neg)\n",
        "    print(\" \")\n",
        "    print(\"SMOOTHED UNIGRAM  : \")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Smoothed Unigram Perplexity (Positive Test Corpus):\", smoothed_unigram_perplexity_pos)\n",
        "    print(\"Smoothed Unigram Perplexity (Negative Test Corpus):\", smoothed_unigram_perplexity_neg)\n",
        "    print(\" \")\n",
        "    print(\"BIGRAM : \")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Bigram Perplexity (Positive Test Corpus):\", bigram_perplexity_pos)\n",
        "    print(\"Bigram Perplexity (Negative Test Corpus):\", bigram_perplexity_neg)\n",
        "    print(\" \")\n",
        "    print(\"SMOOTHED BIGRAM : \")\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Smoothed Bigram Perplexity (Positive Test Corpus):\", smoothed_bigram_perplexity_pos)\n",
        "    print(\"Smoothed Bigram Perplexity (Negative Test Corpus):\", smoothed_bigram_perplexity_neg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kjWepNOqRyl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c252bf-d382-4423-8561-6408db250eaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file  train.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading sentence 1000\n",
            "Reading sentence 2000\n",
            "Reading sentence 3000\n",
            "Reading sentence 4000\n",
            "Reading sentence 5000\n",
            "Reading sentence 6000\n",
            "Reading sentence 7000\n",
            "Reading sentence 8000\n",
            "Reading sentence 9000\n",
            "Reading sentence 10000\n",
            "Reading sentence 11000\n",
            "Reading sentence 12000\n",
            "Reading sentence 13000\n",
            "Reading sentence 14000\n",
            "Reading sentence 15000\n",
            "Reading sentence 16000\n",
            "Reading sentence 17000\n",
            "Reading sentence 18000\n",
            "Reading sentence 19000\n",
            "Reading sentence 20000\n",
            "Reading sentence 21000\n",
            "Reading sentence 22000\n",
            "Reading sentence 23000\n",
            "Reading sentence 24000\n",
            "Reading sentence 25000\n",
            "Reading sentence 26000\n",
            "Reading sentence 27000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file  pos_test.txt\n",
            "Reading file  neg_test.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading sentence 28000\n",
            "Reading sentence 29000\n",
            "Reading sentence 30000\n",
            "Reading sentence 1000\n",
            "Reading sentence 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "UNIGRAM : \n",
            "------------------------------------------------------------------\n",
            "Unigram Perplexity (Positive Test Corpus): 516.6333952693551\n",
            "Unigram Perplexity (Negative Test Corpus): 497.90085561478963\n",
            " \n",
            "SMOOTHED UNIGRAM  : \n",
            "------------------------------------------------------------------\n",
            "Smoothed Unigram Perplexity (Positive Test Corpus): 406.1064967347534\n",
            "Smoothed Unigram Perplexity (Negative Test Corpus): 388.0202224857519\n",
            " \n",
            "BIGRAM : \n",
            "------------------------------------------------------------------\n",
            "Bigram Perplexity (Positive Test Corpus): 79.81613647736447\n",
            "Bigram Perplexity (Negative Test Corpus): 80.69852371189012\n",
            " \n",
            "SMOOTHED BIGRAM : \n",
            "------------------------------------------------------------------\n",
            "Smoothed Bigram Perplexity (Positive Test Corpus): 52.1718508581214\n",
            "Smoothed Bigram Perplexity (Negative Test Corpus): 53.389779919118105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_0jG1PsnqL16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions"
      ],
      "metadata": {
        "id": "ScITwGFHttOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1**"
      ],
      "metadata": {
        "id": "Yl_cH952tzM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram predicts the probability of each indivual words where as bigrams predict the probability of the next word based on the previous word"
      ],
      "metadata": {
        "id": "w4o0K5HstvT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2**"
      ],
      "metadata": {
        "id": "dStLXvg_uLnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Certainly! The models are designed to assign notably different probabilities to sentences. Unigram and smoothed unigram models may favor sentences with frequently occurring words,\n",
        "# whereas bigram and smoothed bigram models take into account word dependencies, leading to diverse probability distributions."
      ],
      "metadata": {
        "id": "H0IxQdx-uO6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3**"
      ],
      "metadata": {
        "id": "-sjaxYTUucZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram models are more effective as they add a touch of accuracy and is more likely to suggest better sentence creation."
      ],
      "metadata": {
        "id": "3njFsY0nug3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4**"
      ],
      "metadata": {
        "id": "KQtnUpJ4u9-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unigram Model:**\n",
        "\n",
        "Positive Test Corpus: 516.63\n",
        "\n",
        "Negative Test Corpus: 497.90\n",
        "\n",
        "**Smoothed Unigram Model (Laplace Smoothing):**\n",
        "\n",
        "Positive Test Corpus: 406.11\n",
        "\n",
        "Negative Test Corpus: 388.02\n",
        "\n",
        "**Bigram Model:**\n",
        "\n",
        "Positive Test Corpus: 79.82\n",
        "\n",
        "Negative Test Corpus: 80.70\n",
        "\n",
        "**Smoothed Bigram Model (Linear Interpolation Smoothing):**\n",
        "\n",
        "Positive Test Corpus: 52.17\n",
        "\n",
        "Negative Test Corpus: 53.39\n",
        "\n",
        "These perplexity values quantify how well each model aligns with the respective test corpora. Lower perplexity values indicate better performance, suggesting that the model is more adept at predicting the given data."
      ],
      "metadata": {
        "id": "JjcAD7Javeec"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I78L-QI4vAtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}