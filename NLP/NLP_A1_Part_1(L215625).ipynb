{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Regular Expressions and Preprocessing"
      ],
      "metadata": {
        "id": "R3PHmtDawq7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1"
      ],
      "metadata": {
        "id": "E9yyAc9OwlNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q-k4bohwjrW",
        "outputId": "8874217b-f1aa-45da-c563-d30bdbe4d05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular expression: [a-zA-Z]+\n",
            "{The} {quick} {brown} {fox} {jumps} {over} {the} {lazy} {dog}. 123 {is} {a} {number}, {but} {so} {is} 3.14! {Pot}, {peat}, {pit}, {put}. {Apple}, {aardvark}, {Zebra}. {Rhythm}, {cat}, {dog}.\n",
            "\n",
            "\n",
            "Regular expression: [A-Z][a-z]*\n",
            "{The} quick brown fox jumps over the lazy dog. 123 is a number, but so is 3.14! {Pot}, peat, pit, put. {Apple}, aardvark, {Zebra}. {Rhythm}, cat, dog.\n",
            "\n",
            "\n",
            "Regular expression: p[aeiou]{,2}t\n",
            "The quick brown fox jumps over the lazy dog. 123 is a number, but so is 3.14! Pot, {peat}, {pit}, {put}. Apple, aardvark, Zebra. Rhythm, cat, dog.\n",
            "\n",
            "\n",
            "Regular expression: \\d+(\\.\\d+)?\n",
            "The quick brown fox jumps over the lazy dog. {123} is a number, but so is {3.14}! Pot, peat, pit, put. Apple, aardvark, Zebra. Rhythm, cat, dog.\n",
            "\n",
            "\n",
            "Regular expression: ([^aeiou][aeiou][^aeiou])*\n",
            "{}T{he }{}q{}u{}i{}c{}k{} {}b{row}{}n{} {fox}{} {jum}{}p{}s{ ov}{}e{}r{} {}t{he laz}{}y{} {dog}{}.{} {}1{}2{}3{ is a number}{},{} {but}{} {so }{}i{}s{} {}3{}.{}1{}4{}!{} {Pot}{},{} {}p{}e{}a{}t{},{} {pit}{},{} {put}{}.{} {}A{}p{}p{le,}{} {}a{}a{}r{}d{var}{}k{},{} {Zebra.}{} {}R{}h{}y{}t{}h{}m{},{} {cat}{},{} {dog}{}.{}\n",
            "\n",
            "\n",
            "Regular expression: \\w+|[^\\w\\s]+\n",
            "{The} {quick} {brown} {fox} {jumps} {over} {the} {lazy} {dog}{.} {123} {is} {a} {number}{,} {but} {so} {is} {3}{.}{14}{!} {Pot}{,} {peat}{,} {pit}{,} {put}{.} {Apple}{,} {aardvark}{,} {Zebra}{.} {Rhythm}{,} {cat}{,} {dog}{.}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "# Define the regular expressions\n",
        "regexes = [\n",
        "    r'[a-zA-Z]+',\n",
        "    r'[A-Z][a-z]*',\n",
        "    r'p[aeiou]{,2}t',\n",
        "    r'\\d+(\\.\\d+)?',\n",
        "    r'([^aeiou][aeiou][^aeiou])*',\n",
        "    r'\\w+|[^\\w\\s]+'\n",
        "]\n",
        "\n",
        "# Test the regular expressions on a sample string\n",
        "text = \"The quick brown fox jumps over the lazy dog. 123 is a number, but so is 3.14! Pot, peat, pit, put. Apple, aardvark, Zebra. Rhythm, cat, dog.\"\n",
        "for regex in regexes:\n",
        "    print(f\"Regular expression: {regex}\")\n",
        "    nltk.re_show(regex, text)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2"
      ],
      "metadata": {
        "id": "54-zfxB3x4Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "# Define the regular expressions\n",
        "determiner_regex = r'\\b(a|an|the)\\b'\n",
        "arithmetic_regex = r'^\\d+([\\*+]\\d+)*$'\n",
        "\n",
        "# Test the regular expressions on sample strings\n",
        "nltk.re_show(determiner_regex, \"This is a test sentence with the word the in it.\")\n",
        "nltk.re_show(arithmetic_regex, \"2*3+8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3KH-umLx5CK",
        "outputId": "ca769ad7-e054-45ba-a744-8fdb7f72415a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is {a} test sentence with {the} word {the} in it.\n",
            "{2*3+8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3"
      ],
      "metadata": {
        "id": "7IkzCQqtyQ4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_plain_text_from_url(url):\n",
        "    html = request.urlopen(url).read().decode('utf8')\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    plain_text = soup.get_text()\n",
        "    return plain_text"
      ],
      "metadata": {
        "id": "0r5dJ3I9yKGU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://apify.com/anchor/email-phone-extractor'\n",
        "plain_text = get_plain_text_from_url(url)\n",
        "print(plain_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAhC7ae3yli6",
        "outputId": "7d2a7be2-1d61-4fe9-b7b4-46ff934779d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email ‚úâÔ∏è  & Phone ‚òéÔ∏è Extractor ¬∑ Apify\n",
            "ü§ñ Get data to feed your AI models, LLMs or GPTs\n",
            "ProductApify StoreStart web scraping with ready-made scrapersCrawleeOur reliable open-source web scraping libraryCode templatesGet started with templates for your scraping projectActorsRun serverless cloud programs on the Apify platformIntegrationsSeamlessly connect with other apps and servicesProxyImprove your web scraping performanceStorageSpecialized cloud storage for web scraping and crawlingApify CLICreate, develop, build, and run Apify actors locallySolutionsDELIVERED BYApify EnterpriseCertified PartnersFOR DEVELOPERSMonetize your ActorsRun Scrapy in the cloudUSE CASESData for generative AI & LLMProduct matching AIUniversal web scrapersAll use casesINSPIRATIONSuccess storiesResourcesHelp and supportGet advice and answers about the Apify platformSubmit your ideasUpvote or submit actor or integration ideasLEARNDocumentationAbout ApifyBlogWeb scraping courseApify platform courseDiscordDocsPricingGet custom solutionLog inEmail ‚úâÔ∏è  & Phone ‚òéÔ∏è ExtractorTry for free7 days trial then $30.00/month - No credit card required now View all ActorsEmail ‚úâÔ∏è  & Phone ‚òéÔ∏è Extractoranchor/email-phone-extractorTry for free7 days trial then $30.00/month - No credit card required nowExtract emails, phone numbers, and other useful contact information from any list of websites you provide. Lets you specify URL patterns for your bot to follow. Export your data in structured formats and use it anywhere.READMEInputAPIWhat does Email ‚úâÔ∏è & Phone ‚òéÔ∏è Extractor do?\n",
            "This email and phone extractor allows you to scrape any website for contact information. That's right - any website üí™\n",
            "Just enter a URL, and the crawler will begin to extract info and click on links (‚Äúa‚Äù anchors). These are some of the types of information that this contact details crawler can extract:\n",
            "\n",
            "Email\n",
            "Phone numbers\n",
            "LinkedIn\n",
            "Twitter\n",
            "Instagram\n",
            "Facebook\n",
            "\n",
            "Why do I need this scraper?\n",
            "Email ‚úâÔ∏è & Phone ‚òéÔ∏è Extractor might be handy for your marketing and sales teams, whether you are just getting started with your company and looking to build up a database of potential clients or you are an established business and want to grow your network.\n",
            "How much will this scraper cost me?\n",
            "The Apify Free plan will give you 10,000 results, the Personal plan will give you 100,000 results, and the Team plan will deliver 1 million results!\n",
            "Check out Apify pricing to see which plan is right for you.\n",
            "How to scrape contact details from any website\n",
            "If you want to read more about this scraper, its functionalities, and its setup, you can find a step-by-step tutorial.\n",
            "Personal data and the legality of scraping contact information\n",
            "You should be aware that your results might contain personal data. Personal data is protected by GDPR in the European Union and by other regulations around the world. You should not scrape personal data unless you have a legitimate reason to do so. If you're unsure whether your reason is legitimate, consult your lawyers. You can also read this Apify blog post on the legality of web scraping.\n",
            "What will the results look like?\n",
            "This actor stores its results into the default dataset associated with the actor run. You can then download the results in formats such as JSON, HTML, CSV, XML, or Excel. For each page crawled, the following contact information is extracted (examples shown):\n",
            "\n",
            "Emails\n",
            "1alice@example.com\n",
            "2bob.newman@example.com\n",
            "3carl+test@example.co.uk\n",
            "\n",
            "Phone numbers - These are extracted from phone links in HTML (e.g. <a href='tel://123456789'>phone</a>).\n",
            "1123456789\n",
            "2+33652394840\n",
            "300123456789\n",
            "\n",
            "Uncertain phone numbers - These are extracted from the plain text of the web page using a number of regular expressions. Note that this approach can generate false positives.\n",
            "1+123.456.7890\n",
            "206 52 39 48 40\n",
            "3123-456-789\n",
            "\n",
            "LinkedIn profiles\n",
            "1https://www.linkedin.com/in/alan-turing\n",
            "2en.linkedin.com/in/alan-turing\n",
            "3linkedin.com/in/alan-turing\n",
            "\n",
            "Twitter profiles\n",
            "1https://www.twitter.com/apify\n",
            "2twitter.com/apify\n",
            "\n",
            "Instagram profiles\n",
            "1https://www.instagram.com/old_prague\n",
            "2www.instagram.com/old_prague/\n",
            "3instagr.am/old_prague\n",
            "\n",
            "Facebook profiles or pages\n",
            "1https://www.facebook.com/apifytech\n",
            "2facebook.com/apifytech\n",
            "3fb.com/apifytech\n",
            "4https://www.facebook.com/profile.php?id=123456789\n",
            "\n",
            "DeveloperguillimMaintained by CommunityActor metrics 187 monthly users 99.3% runs succeeded 5.5 days response time Modified 9 months agoCategoriesMarketingLead generationBusinessYou might also like these ActorsGoogle Maps Scraper compass/crawler-google-placesExtract data from hundreds of Google Maps locations and businesses. Get Google Maps data including reviews, images, contact info, opening hours, location, popular times, prices & more. Export scraped data, run the scraper via API, schedule and monitor runs, or integrate with other tools.Compass52.9kGoogle Search Results Scraper apify/google-search-scraperScrape Google Search Engine Results Pages (SERPs). Select the country or language and extract organic and paid results, ads, queries, People Also Ask, prices, reviews, like a Google SERP API. Export scraped data, run the scraper via API, schedule and monitor runs, or integrate with other tools.Apify40kFacebook Posts Scraper apify/facebook-posts-scraperExtract data from hundreds of Facebook posts from one or multiple Facebook pages and profiles. Get post URL, post text, page or profile URL, timestamp, number of likes, shares, comments, and more. Download the data in JSON, CSV, and Excel and use it in apps, spreadsheets, and reports.Apify4.9kInstagram Hashtag Scraper apify/instagram-hashtag-scraperScrape Instagram hashtags data. Just add one or more hashtags and extract posts, images, URLs, comments, likes, users, locations, timestamps, and more. Export scraped datasets, run the scraper via API, schedule and monitor runs or integrate with other tools.Apify10.4kWebsite Content Crawler apify/website-content-crawlerAutomatically crawl and extract text content from websites with documentation, knowledge bases, help centers, or blogs. This Actor is designed to provide data to feed, fine-tune, or train large language models such as ChatGPT or LLaMA.Apify9kContact Details Scraper vdrmota/contact-info-scraperFree email extractor to extract and download emails, phone numbers, Facebook, Twitter, LinkedIn, and Instagram profiles from any website. Extract contact information at scale from lists of URLs and download the data as Excel, CSV, JSON, HTML, and XML.Vojta Drmota14.9kInstagram Profile Scraper apify/instagram-profile-scraperScrape all Instagram profile info. Just add one or more Instagram usernames and extract number of followers&follows, URLs, bio, posts, likes, counts, related profiles, captions, highlight reels. Export scraped data, run the scraper via API, schedule and monitor runs or integrate with other tools.Apify23.6kTwitter Scraper quacker/twitter-scraperScrape tweets from any Twitter user profile. Top Twitter API alternative to scrape Twitter hashtags, threads, replies, followers, images, videos, statistics, and Twitter history. Export scraped data, run the scraper via API, schedule and monitor runs or integrate with other tools.Quacker21.3kGPT Scraper drobnikj/gpt-scraperExtract data from any website and feed it into GPT via the OpenAI API. Use ChatGPT to proofread content, analyze sentiment, summarize reviews, extract contact details, and much more.Jakub Drobn√≠k3.4kAI Product Matcher equidem/ai-product-matcherMatch products across multiple e-commerce websites. Use this AI product matching Actor whenever you need to find matching pairs of products from different online shops for dynamic pricing, competitor analysis or market research.Matƒõj Sochor177Related articles11 email scrapers you should know aboutRead moreTop 10 web scrapers for lead generationRead moreHow to extract emails, phone numbers, and social profiles from websitesRead moreWhere next?Build new toolsAre you a developer? Build your own Actors and run them on Apify.Enterprise solutionsGet a complete web scraping or automation solution from Apify experts.Join our developer community on DiscordCookie settingsTerms of usePrivacy policyCookie policy¬© 2024 Apify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4"
      ],
      "metadata": {
        "id": "Fgeq5gQZzCNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from urllib import request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_phone_numbers_and_emails_from_url(url):\n",
        "    html = request.urlopen(url).read().decode('utf8')\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    plain_text = soup.get_text()\n",
        "    phone_numbers = re.findall(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', plain_text)\n",
        "    emails = re.findall(r'\\b[\\w.-]+@[\\w.-]+\\b', plain_text)\n",
        "    return phone_numbers, emails"
      ],
      "metadata": {
        "id": "m_59ByZBypaZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://apify.com/anchor/email-phone-extractor'\n",
        "phone_numbers, emails = get_phone_numbers_and_emails_from_url(url)\n",
        "print(\"Phone numbers:\", phone_numbers)\n",
        "print(\"Emails:\", emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aIsCq72zHV0",
        "outputId": "53292e03-ff6c-4bba-e830-28d9df4c31c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phone numbers: ['1123456789', '123.456.7890']\n",
            "Emails: ['1alice@example.com', '2bob.newman@example.com', 'test@example.co.uk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5"
      ],
      "metadata": {
        "id": "Rl7_9duVzz8R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4HUWPL005Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZaZty8z046A",
        "outputId": "d4fa7f59-be6e-4bbc-f2f0-64b51eebddd3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample tokenized text\n",
        "tokenized_text = word_tokenize(\"We are eating our lunch at the meeting\")\n",
        "\n",
        "# Using Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "porter_stemmed = [porter.stem(word) for word in tokenized_text]\n",
        "print(\"Porter Stemmer:\", porter_stemmed)\n",
        "\n",
        "# Using Lancaster Stemmer\n",
        "lancaster = LancasterStemmer()\n",
        "lancaster_stemmed = [lancaster.stem(word) for word in tokenized_text]\n",
        "print(\"Lancaster Stemmer:\", lancaster_stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAr2zBvSzLpC",
        "outputId": "f7896f35-e991-4e6e-cc3a-6670dbff2c06"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['we', 'are', 'eat', 'our', 'lunch', 'at', 'the', 'meet']\n",
            "Lancaster Stemmer: ['we', 'ar', 'eat', 'our', 'lunch', 'at', 'the', 'meet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6"
      ],
      "metadata": {
        "id": "xbTbRBwb0-WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "text = \"Pila Forfeited you engrossed but 1kometimes explained. Another 1kacokaco1 as studied it to evident. Merry sense 9given he be arisepila. Conduct at an replied removal an amongst. Remainingzalima 0determine few her two cordially Zalima admitting old. Sometimes ctra*nger his pisdsdla ourselves her co*la depending you boy. Eat discretion cultivated possession far comparison projection pila considered. And few fat interested discovered inquietude insensible unsatiable increasing zalima eat.\"\n",
        "\n",
        "# Regular expressions\n",
        "first_word_regex = r\"^[Zz][a-z]*a\"\n",
        "second_word_regex = r\"\\d[kK][a-z]*\\d\"\n",
        "third_word_regex = r\"c[a-z]*\\*[a-z]+a\"\n",
        "fourth_word_regex = r\"[Pp][a-z]{2}a\"\n",
        "\n",
        "# Extracting words\n",
        "first_word = re.findall(first_word_regex, random_text)\n",
        "second_word = re.findall(second_word_regex, random_text)[0][1:-1]\n",
        "third_word = re.findall(third_word_regex, random_text)[0][1:-1]\n",
        "fourth_word = re.findall(fourth_word_regex, random_text)\n",
        "#message = {first_word[0]} {second_word} {third_word} {fourth_word[0]}"
      ],
      "metadata": {
        "id": "QA6IVAQo0_p8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Given random text\n",
        "random_text = \"Pila Forfeited you engrossed but 1kometimes explained. Another 1kacokaco1 as studied it to evident. Merry sense 9given he be arisepila. Conduct at an replied removal an amongst. Remainingzalima 0determine few her two cordially Zalima admitting old. Sometimes ctra*nger his pisdsdla ourselves her co*la depending you boy. Eat discretion cultivated possession far comparison projection pila considered. And few fat interested discovered inquietude insensible unsatiable increasing zalima eat.\"\n",
        "\n",
        "# Regular expressions\n",
        "first_word_regex = r\"\\b[Zz][a-z]*a\\b\"\n",
        "second_word_regex = r\"\\b\\d[kK][a-z]*\\d\\b\"\n",
        "third_word_regex = r\"\\bc[a-z]*\\*[a-z]+a\\b\"\n",
        "fourth_word_regex = r\"\\b[Pp][a-z]{2}a\\b\"\n",
        "\n",
        "# Extracting words\n",
        "first_word = re.findall(first_word_regex, random_text)\n",
        "second_word = [word[1:-1] for word in re.findall(second_word_regex, random_text)]\n",
        "third_word = [word.replace('*', '') for word in re.findall(third_word_regex, random_text)]\n",
        "fourth_word = re.findall(fourth_word_regex, random_text)\n",
        "\n",
        "# Constructing the message\n",
        "message = f\"{first_word[0]} {second_word[0]} {third_word[0]} {fourth_word[0]} de\"\n",
        "\n",
        "print(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA_JDEHY4cIt",
        "outputId": "c453181b-0950-44e3-dc46-01ef589a7576"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zalima kacokaco cola Pila de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27T1S__s4cvu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}